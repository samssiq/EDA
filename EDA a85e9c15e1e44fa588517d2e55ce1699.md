# EDA

### AnÃ¡lise de Algoritmos (introduÃ§Ã£o)

O que significa analisar algoritmos?

Ao analisar um algoritmo, tenta-se prever, ou, pelo menos, ter uma ideia da quantidade de recursos que serÃ¡ utilizada ao executÃ¡-lo. Isso pode ser feito em funÃ§Ã£o de vÃ¡rias coisas e dentre elas estÃ¡ o tempo de execuÃ§Ã£o, uma variÃ¡vel muito utilizada.

Assim, dependendo do algoritmo utilizado, a quantidade de entradas pode aumentar significativamente o tempo de execuÃ§Ã£o, enquanto em outros o tempo de execuÃ§Ã£o nÃ£o Ã© tÃ£o drasticamente afetado mesmo que a quantidade de entradas seja muito maior. 

No grÃ¡fico abaixo, o Selection sorte fica bem mais lento quando as entradas aumentam, diferente do que acontece com os outros algoritmos listados.

![https://joaoarthurbm.github.io/eda/posts/introducao-a-analise/comparacao-ordenacao.jpeg](https://joaoarthurbm.github.io/eda/posts/introducao-a-analise/comparacao-ordenacao.jpeg)

HipÃ³tese base da anÃ¡lise de algoritmos e operaÃ§Ãµes primitivas

> HipÃ³tese: O custo de operaÃ§Ãµes primitivas Ã© constante.
> 

Em tese, todas as operaÃ§Ãµes primitivas apresentadas abaixo possuem o mesmo tempo de execuÃ§Ã£o. Entretanto, na vida real, esse custo pode variar de acordo com a mÃ¡quina, a linguagem etc, mas a variaÃ§Ã£o Ã© insignificante.

**OperaÃ§Ãµes Primitivas**

- AvaliaÃ§Ã£o de expressÃµes booleanas (i >= 2; i == 2, etc);
- OperaÃ§Ãµes matemÃ¡ticas (*, -, +, %, etc);
- Retorno de mÃ©todos (return x;);
- AtribuiÃ§Ã£o (i = 2);
- Acesso Ã  variÃ¡veis e posiÃ§Ãµes arbitrÃ¡rias de um array (v[i]).

Num algoritmo simples, o custo Ã© totalizado com 3 passos simples:

1. Identificar primitivas 
2. Identificar seu nÃºmero de execuÃ§Ãµes                               
3. Somar o custo total.

> Dizer que um algoritmo tem custo constante significa dizer que o seu tempo de execuÃ§Ã£o independe do tamanho da entrada.
> 

Com base nisso, o custo de operaÃ§Ã£o serÃ¡ a soma das primitivas, PORÃ‰M, a medida que os algoritmos ficam mais refinados, esse custo comeÃ§a a levar em consideraÃ§Ã£o outras coisas.

Condicionais e o pior caso 

O pior caso Ã© o mais comum de acontecer. Fato. Quando pensamos nessa possibilidade, jÃ¡ eliminamos soluÃ§Ãµes ruins. Num algoritmo que calcula a aprovaÃ§Ã£o de um aluno, por exemplo, o pior caso serÃ¡ a reprovaÃ§Ã£o. Se isso acontece, uma parte do cÃ³digo menos signficativa deixa de ser executada. Como hÃ¡ uma condicional, isso muda o passo 2, pois primitivas podem ser excluidas, somadas, duplicadas, etc no processo.

IteraÃ§Ã£o

A iteraÃ§Ã£o afeta o nÃºmero de vezes que uma primitiva Ã© executada. Ela pode ser n vezes, ou n + 1 vezes, talvez.

Com isso o valor final nÃ£o Ã© mais apenas a soma das primitivas e agora leva em consideraÃ§Ã£o o numero de vezes que elas sÃ£o executadas.

Loops aninhados

Devemos analisar com CALMA quantas vezes cada primitiva vai ser executada e levar em consideraÃ§Ã£o os loops internos e sua relaÃ§Ã£o de dependÃªncia com os mais externos.

### AnÃ¡lise AssintÃ³tica

Ordem de crescimento

Ao analisar uma funÃ§Ã£o, pode-se abstrair algumas informaÃ§Ãµes e ainda assim ser possÃ­vel identificar sua ordem de crescimento, o que Ã© bastante Ãºtil quando trata-se de algoritmos com valores altos de entrada. 

Como isso Ã© feito?

- Ignorando constantes
- Ignorando expoentes de menor magnitude

Mas por que esses valores podem ser ignorados? Simplesmente pelo fato deles serem insignificantes quando comparados com um nÂ² ou nÂ³, por exemplos. Ou seja: nÃ£o fazem diferenÃ§a real numa funÃ§Ã£o que trabalha com valores altos.

Exemplo

$f(n)=3c + 2c n+3âˆ—n2/2+câˆ—(n2+n)/2$

Realizando os processos de abstraÃ§Ã£o, podemos dizer que a funÃ§Ã£o acima possui uma ordem de crescimento de nÂ², ou seja, Ã© uma funÃ§Ã£o quadrÃ¡tica.

> Quando observamos tamanhos de entrada grande o suficiente para tornar relevante apenas a ordem de crescimento do tempo de execuÃ§Ã£o, estamos estudando a eficiÃªncia assintÃ³tica.
> 

**NotaÃ§Ã£o Theta  $Î˜$**

De acordo com o material utilizado com base neste resumo, aka material do prof JoÃ£o Arthur: 

> Para duas funÃ§Ãµes $f(n$) e $g(n)$, dizemos que $f(n)$ Ã© $Î˜(g(n))$ se $0â‰¤c1âˆ—g(n)â‰¤f(n)â‰¤c2âˆ—g(n),âˆ€nâ‰¥n0$
> 

Mas o que isso quer dizer? Basicamente, que o crescimento da nossa funÃ§Ã£o f(n) Ã© igual ao da funÃ§Ã£o g(n). Vou demonstrar isso usando uma funÃ§Ã£o bem simples: $f(n) = 3n + 3.$

**Passo 1:** Vamos abstrair a funÃ§Ã£o 3n + 3.

3n + 3 passa a ser apenas n, ou seja, uma funÃ§Ã£o linear e agora assumirÃ¡ o valor de g(n). O valor de f(n) permanecerÃ¡ o mesmo.

0 â‰¤ c1 * n â‰¤ 3n + 3 â‰¤ c2 * n (p/ todo e qualquer valor de n maior ou igual a zero).

**Passo 2:** Estabelecer um valor p/ n respeitando o que foi dito acima. Dessa forma, diremos que n = 1.

**Passo 3:** Resolver f(n) com o valor de n estabelecido.

0 â‰¤ c1 * n â‰¤ 3n + n â‰¤ c2 * n  $â†’$ 0 â‰¤ c1 * 1 â‰¤ 3 * 1 + 3  â‰¤ c2 * 1

$â†’$ 0 â‰¤ c1 â‰¤ 6 â‰¤ c2. 

**Passo 4:** Definir valores para c1 e c2. c1 deve ser maior que zero e menor ou igual a 6 e c2 deve ser maior ou igual a 6 no exemplo acima.

Se c1 = 1 e c2 = 6, ou, sei lÃ¡, c1 = 2 e c2 = 7, o valor de f(n) continua sendo maior que c1 e menor que c2.

Assim: 0 â‰¤ 2 â‰¤ 6 â‰¤ 7. Sendo 2 o limite inferior e 7 e o limite superior da funÃ§Ã£o.

**NotaÃ§Ã£o O (Big O notation)**

Define um limite superior para uma funÃ§Ã£o f(n). Segundo o material do prof JoÃ£o Arthur:

> Para duas funÃ§Ãµes $f(n)$ e $g(n)$, dizemos que $f(n)$ Ã© $O(g(n))$ se: $0â‰¤f(n)â‰¤câˆ—g(n),âˆ€nâ‰¥n0$
> 

Acho que vocÃª percebeu que essa inequaÃ§Ã£o Ã© bem parecida com a da notaÃ§Ã£o Theta, certo? A diferenÃ§a entre elas Ã© que o limite inferior da Big O Ã© zero, alÃ©m de trabalharmos apenas com um c (limite superior) a ser definido.

Os passos para definir o limite superior da Big O sÃ£o os mesmos da notaÃ§Ã£o Theta. Assim, dizemos que o limite superior da funÃ§Ã£o f(n) = nÂ² + 1 Ã© 2.

Toda funÃ§Ã£o Theta Ã© uma Big O, mas nem toda Big O Ã© uma Theta, visto que Thetas possuem limite inferior e Big Os nÃ£o.

<aside>
ğŸ’¡ â€œ[...] basta escolhermos uma funÃ§Ã£o com **n** elevado a um expoente maior do que o da funÃ§Ã£o sob anÃ¡lise que conseguimos definir um limite superior para ele. Por exemplo, a funÃ§Ã£o f(n)=nÂ² Ã© O(nÂ²), O(nÂ³), O(n4) [â€¦]â€  Um exemplo Ã© a funÃ§Ã£o f(n) = 7n. Seu limite superior pode ser n, nÂ², nÂ³, etcâ€¦

</aside>

> â€œEm termos simplistas, f(n)âˆˆO(g(n)) significa dizer que o crescimento de f(n) Ã© menor ou igual ao crescimento de g(n).â€
> 

**NotaÃ§Ã£o Omega (Î©)**

Define o limite inferior para uma funÃ§Ã£o f(n). Assim:

> Para duas funÃ§Ãµes $f(n)$ e $g(n)$, dizemos que $f(n)$ Ã© $Î©(g(n))$ se: $0â‰¤câˆ—g(n)â‰¤f(n),âˆ€nâ‰¥n0$
> 

O processo para achar seu limite inferior Ã© o mesmo da notaÃ§Ã£o Theta. 

Toda funÃ§Ã£o Theta Ã© Omega, mas nem toda Omega Ã© Theta, pois a Omega nÃ£o possui limite superior.

<aside>
ğŸ’¡ Todo limite inferior de Omega pode ser 1, basta usar o expoente 0. AlÃ©m disso, dado um expoente, para limitÃ¡-lo inferiormente basta escolher um expoente menor. Por exemplo, f(nÂ³) Ã© Î©(nÂ²).

</aside>

> Em termos simplistas, f(n)âˆˆÎ©(g(n)) significa dizer que o crescimento de f(n) Ã© maior ou igual ao crescimento de g(n).
>